{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Introduction"
      ],
      "metadata": {
        "id": "9Ygu8qUb3apR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Using Hugging Face models**"
      ],
      "metadata": {
        "id": "L2n7YhLo3iFa"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e1qfta-n2xy1"
      },
      "outputs": [],
      "source": [
        "from transformers import pipeline\n",
        "summarizer = pipeline(task=\"summarization\", model=\"facebook/bart-large-cnn\")\n",
        "\n",
        "text =\n",
        "\"Walking amid Gion's Machiya wooden houses is a mesmerizing experience. The beautifully preserved structures exuded an old-world charm that transports visitors back in time, making them feel like they had stepped into a living museum. The glow of  laterns lining the narrow streets add to the enchanting ambiance, making each stroll a memorable journey through Japan's rich cultural history.\"\n",
        "\n",
        "\n",
        "summary = summarizer(text, max_length=50)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "Depending on the model, tokenizer, or text, we may end up with unwanted whitespace in our output.\n",
        "We can remove this by adding the argument clean_up_tokenization_spaces to the pipeline and setting it to True. Most of today's summarization models do this automatically\n",
        "\n",
        "\n",
        "summarizer(text, clean_up_tokenization_spaces=True)\n",
        "\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "\n",
        "Load the model pipeline for a summarization task using the model \"cnicu/t5-small-booksum\".\n",
        "Generate the output by passing the long_text to the pipeline; limit the output to 50 tokens.\n",
        "Access and print the summarized text only from the output\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "# Load the model pipeline\n",
        "summarizer = pipeline(task = \"summarization\", model=\"cnicu/t5-small-booksum\")\n",
        "\n",
        "# Pass the long text to the model\n",
        "output = summarizer(long_text, max_length = 50)\n",
        "\n",
        "# Access and print the summarized text\n",
        "print(output[0][\"summary_text\"])"
      ],
      "metadata": {
        "id": "jp4Wgg7M4RVI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Using pre-trained LLMs"
      ],
      "metadata": {
        "id": "w8IrCba04rRU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Text Generation**"
      ],
      "metadata": {
        "id": "pZrgWZRD5V31"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "\n",
        "The pad_token_id parameter fills in extra space up to the specified max_length through padding, which adds extra tokens to make all sequences the same length,\n",
        "ensuring model efficiency. Setting this parameter to the tokenizer's end-of-sequence token ID, learned through training, marks the end of meaningful text.\n",
        "This setting helps the model recognize where to stop generating, ensuring it only produces text up to the specified length or the end-of-sequence token.\n",
        "Another parameter, truncation=True, can be added if the input is longer than the maximum length we have set. We won't need it for this example.\n",
        "\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "IKe9U1iA4uD3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "\n",
        "The output for this model is retrieved using the generated_text key. Sometimes, the output may be suboptimal if the prompt lacks context.\n",
        "For instance, if the prompt is too vague or ambiguous, the generated text might not be relevant or coherent.\n",
        "\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "DB6pQbPHN9vo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Guiding the Output**"
      ],
      "metadata": {
        "id": "dKerSquGOjR0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "\n",
        "We can control the output by being more specific in the prompt or including additional elements to guide the output. Take this book review example.\n",
        "We've included a response element and combined the review and response into a single prompt using an f-string to guide the model.\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "generator = pipeline(task=\"text-generation\", model=\"distilgpt2\")\n",
        "review = \"This book was great. I enjoyed the plot twist in Chapter 10.\"\n",
        "response = \"Dear reader, thank you for your review.\"\n",
        "prompt = f\"Book review:\\n {review}\\n\\nBook shop response to the review:\\n{response}\"\n",
        "print(output[0][\"generated_text\"])\n",
        "output = generator(prompt, max_length=100, pad_token_id=generator.tokenizer.eos_token_id)"
      ],
      "metadata": {
        "id": "Z8tPKfT0Olsp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Language Translation**"
      ],
      "metadata": {
        "id": "kRW-0anOQKJ7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "translator = pipeline(task=\"translation_en_to_es\", model=\"Helsinki-NLP/opus-mt-en-es\")\n",
        "text = \"Walking amid Gion's Machiya wooden houses was a mesmerizing experience.\"\n",
        "output = translator(text, clean_up_tokenization_spaces=True)\n",
        "print(output[0][\"translation_text\"])"
      ],
      "metadata": {
        "id": "_MDtb6xHQMk5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "\n",
        "You need to generate a response to a customer review found in text; it contains the same customer review for the Riverview Hotel you've seen before.\n",
        "The pipeline module has been loaded for you.\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "Instantiate the generator pipeline specifying an appropriate task for generating text.\n",
        "Complete the prompt by including the text and response in the f-string.\n",
        "Complete the model pipeline by specifying a maximum length of 150 tokens and setting the pad_token_id to the end-of-sequence token.\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Instantiate the pipeline\n",
        "generator = pipeline(task = \"text-generation\", model=\"gpt2\")\n",
        "\n",
        "response = \"Dear valued customer, I am glad to hear you had a good stay with us.\"\n",
        "\n",
        "# Complete the prompt\n",
        "prompt = f\"Customer review:\\n{text}\\n\\nHotel reponse to the customer:\\n{response}\"\n",
        "\n",
        "# Complete the model pipeline\n",
        "outputs = generator(prompt, max_length = 150, pad_token_id=generator.tokenizer.eos_token_id, truncation=True)\n",
        "\n",
        "print(outputs[0][\"generated_text\"])\n",
        "\n"
      ],
      "metadata": {
        "id": "hzbpwUzZQcAz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "\n",
        "Define the pipeline task for Spanish-to-English translation (es_to_en).\n",
        "Translate the spanish_text using the model pipeline.\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "spanish_text = \"Este curso sobre LLMs se est√° poniendo muy interesante\"\n",
        "\n",
        "# Define the pipeline\n",
        "translator = pipeline(task=\"translation_es_to_en\", model=\"Helsinki-NLP/opus-mt-es-en\")\n",
        "\n",
        "# Translate the Spanish text\n",
        "translations = translator(spanish_text, clean_up_tokenization_spaces=True)\n",
        "\n",
        "print(translations[0][\"translation_text\"])"
      ],
      "metadata": {
        "id": "1v8z58hsQ6KN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Understanding the Transformer"
      ],
      "metadata": {
        "id": "YO2j4NAfRbFK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### To understand Transformer's architecture\n",
        "\n",
        "llm.model\n",
        "llm.model.config\n",
        "llm.model.config.is_decoder\n",
        "llm.model.config.is_encoder_decoder"
      ],
      "metadata": {
        "id": "WwzaBoNBReus"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "\n",
        "Question-answering can be either extractive or generative, each requiring a different transformer structure to process input and output correctly.\n",
        "\n",
        "They use either:\n",
        "\n",
        "Encoder-only models such as \"distilbert-base-uncased-distilled-squad\"\n",
        "Decoder-only models such as \"gpt2\"\n",
        "Use your knowledge of common models for specific tasks to select the appropriate one. pipeline is loaded, as well as text on the Mona Lisa.\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "Use an appropriate model for extractive question-answering.\n",
        "\"\"\"\n",
        "\n",
        "from transformers import pipeline\n",
        "question = \"Who painted the Mona Lisa?\"\n",
        "\n",
        "qa = pipeline(task=\"question-answering\", model=\"gpt2\")\n",
        "\n",
        "output = qa(question=question, context=text)\n",
        "print(output['answer'])"
      ],
      "metadata": {
        "id": "pyjUO2EwJap5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "\n",
        "Use an appropriate model for generative question-answering.\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "question = \"Who painted the Mona Lisa?\"\n",
        "\n",
        "# Define the appropriate model\n",
        "qa = pipeline(task=\"question-answering\", model=\"gpt2\")\n",
        "\n",
        "input_text = f\"Context: {text}\\n\\nQuestion: {question}\\n\\nAnswer:\"\n",
        "\n",
        "output = qa({\"context\": text, \"question\": question}, max_length=150)\n",
        "print(output['answer'])"
      ],
      "metadata": {
        "id": "cNfxEdyOJ3ZC"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}