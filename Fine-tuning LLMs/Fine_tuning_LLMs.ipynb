{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Preparing for fine-tuning"
      ],
      "metadata": {
        "id": "IhMgQQIzLSlH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Pipelines and auto classes**"
      ],
      "metadata": {
        "id": "7yndXF2NLjlB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BuYafVXELEp-"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "\n",
        "So far, we have used the pipeline() interface. It streamlines language tasks by automatically selecting a model and tokenizer but offers limited control.\n",
        "Auto classes allow more customization, enabling manual adjustments and model fine-tuning\n",
        "\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Tokenization**"
      ],
      "metadata": {
        "id": "MNPfl0ZxeprU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "\n",
        "After loading and instantiating the data, model, and tokenizer, we tokenize the data subset in one go by selecting the text column,\n",
        "enabling padding and sequence truncation when exceeding the specified maximum length. This helps with efficiency.\n",
        "We set return_tensors to pt to return PyTorch tensors since our model expects this format.\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "from datasets import load_dataset\n",
        "\n",
        "\n",
        "train_data = load_dataset(\"imdb\", split=\"train\")\n",
        "train_data = data.shard(num_shards=4, index=0)\n",
        "test_data = load_dataset(\"imdb\", split=\"test\")\n",
        "test_data = data.shard(num_shards=4, index=0)\n",
        "\n",
        "\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\"bert-base-uncased\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "\n",
        "# Tokenize the data\n",
        "tokenized_training_data = tokenizer(train_data[\"text\"], return_tensors=\"pt\", padding=True, truncation=True,\n",
        "                                    max_length=64)\n",
        "tokenized_test_data = tokenizer(test_data[\"text\"], return_tensors=\"pt\", padding=True, truncation=True,\n",
        "                                max_length=64)\n"
      ],
      "metadata": {
        "id": "KS0lBt26erfZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Tokenizing row by row**"
      ],
      "metadata": {
        "id": "0REeK5OXfFSP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "\n",
        "If more control is needed, we can tokenize a dataset in batches or row by row with a custom function and the .map() method,\n",
        "setting batches to True or False, respectively. The result will be a new dataset object with new columns for the tokenized data,\n",
        "which is required for the training loop. Note that the .map() method does not accept list formats, only dataset objects\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "def tokenize_function(text_data):\n",
        "    return tokenizer(text_data[\"text\"], return_tensors=\"pt\", padding=True, truncation=True, max_length=64)\n",
        "\n",
        "# Tokenize in batches\n",
        "tokenized_in_batches = train_data.map(tokenize_function, batched=True)\n",
        "# Tokenize row by row\n",
        "tokenized_by_row = train_data.map(tokenize_function, batched=False)"
      ],
      "metadata": {
        "id": "zrVXnKFkfG9j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Subword tokenization**"
      ],
      "metadata": {
        "id": "g2pmwQnzfsHB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "\n",
        "The tokenization we've performed is known as subword tokenization, common in most modern tokenizers. Here, words are split into smaller,\n",
        "meaningful sub-parts of a word, including prefixes and suffixes. For example, with subword tokenization, a word like \"unbelievably\"\n",
        "would be split into tokens \"un\", \"believ\", and \"ably\".\n",
        "\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "Wlq5ilcsLyto"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "\n",
        "You want to leverage a pre-trained model from Hugging Face and fine-tune it with data from your company support team to help classify interactions depending on the risk for churn. \\\n",
        "This will help the team prioritize what to address first, and how to address it, making them more proactive.\n",
        "\n",
        "Prepare the training and test data for fine-tuning by tokenizing the text\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "Load the pre-trained model and tokenizer in preparation for fine-tuning.\n",
        "Tokenize both the train_data[\"text\"] and test_data[\"text\"], enabling padding and sequence truncation.\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "from datasets import load_dataset\n",
        "\n",
        "\n",
        "train_data = load_dataset(\"imdb\", split=\"train\")\n",
        "train_data = train_data.shard(num_shards=4, index=0) #### Divides the training data into 4 equal parts , index=0 means working with the first slice only, to make the data smaller and faster to process.\n",
        "test_data = load_dataset(\"imdb\", split=\"test\")\n",
        "test_data = test_data.shard(num_shards=4, index=0)\n",
        "\n",
        "# Load the model and tokenizer\n",
        "model =  AutoModelForSequenceClassification.from_pretrained(\"distilbert-base-uncased\")  ### bert-base-uncased: Refers to the specific BERT model. \"Uncased\" means it doesn't differentiate between uppercase and lowercase letters\n",
        "tokenizer =  AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")  #### Automatically loads a tokenizer that matches the model\n",
        "\n",
        "# Tokenize the data\n",
        "tokenized_training_data = tokenizer(train_data[\"text\"], return_tensors=\"pt\", padding = True, truncation = True, max_length=20)\n",
        "\n",
        "tokenized_test_data = tokenizer(test_data[\"text\"], return_tensors=\"pt\", padding = True, truncation = True, max_length=20)\n",
        "\n",
        "\"\"\"\n",
        "tokenizer(train_data[\"text\"]):\n",
        "Takes the text column of the training data and converts it into tokens.\n",
        "\n",
        "return_tensors=\"pt\":\n",
        "Converts the tokens into PyTorch tensors (needed for training the model).\n",
        "\n",
        "padding=True:\n",
        "Ensures all sequences have the same length by adding padding if they are too short.\n",
        "\n",
        "truncation=True:\n",
        "Cuts off sequences that are too long (prevents memory issues).\n",
        "\n",
        "max_length=64:\n",
        "Limits the maximum length of tokens for each sequence to 64 tokens.\n",
        "\"\"\"\n",
        "\n",
        "print(tokenized_training_data)"
      ],
      "metadata": {
        "id": "vKd0UpHeg6Vx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "\n",
        "\n",
        "1. For Sequence Classification (like Sentiment Analysis)\n",
        "----------------------------------------------------------\n",
        "\n",
        "from transformers import AutoModelForSequenceClassification\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\"bert-base-uncased\")\n",
        "\n",
        "--->  This is used when you want to classify sequences into categories (e.g., positive/negative sentiment).\n",
        "--->  It loads a version of BERT with an additional classification head (a fully connected layer for predictions).\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "2. For Token Classification (like Named Entity Recognition)\n",
        "-------------------------------------------------------------\n",
        "from transformers import AutoModelForTokenClassification\n",
        "model = AutoModelForTokenClassification.from_pretrained(\"bert-base-uncased\")\n",
        "\n",
        "---> We Use this for tasks where need predictions at the token level, like labeling each word as a person, location, or organization.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "3. For Question Answering\n",
        "----------------------------\n",
        "from transformers import AutoModelForQuestionAnswering\n",
        "model = AutoModelForQuestionAnswering.from_pretrained(\"bert-base-uncased\")\n",
        "\n",
        "\n",
        "--->We use this for tasks where input is a question and a context, and the model predicts the answer span from the context.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "4. For Language Modeling (Masked or Causal LM)\n",
        "Masked Language Modeling (MLM) (e.g., BERT):\n",
        "--------------------------------------------------------\n",
        "\n",
        "from transformers import AutoModelForMaskedLM\n",
        "model = AutoModelForMaskedLM.from_pretrained(\"bert-base-uncased\")\n",
        "\n",
        "--->  Use this to predict masked words in a sentence, which is how BERT was originally trained.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "5. For Embedding Extraction (Without Task-Specific Heads)\n",
        "---------------------------------------------------------------\n",
        "from transformers import AutoModel\n",
        "model = AutoModel.from_pretrained(\"bert-base-uncased\")\n",
        "\n",
        "\n",
        "--->  This loads BERT without any task-specific layers (just the transformer blocks).\n",
        "--->  Useful when you want to extract embeddings for sentences or tokens\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "6. For Custom Models\n",
        "If need to fine-tune BERT for a unique task, you can load the base BERT model and add your custom head:\n",
        "---------------------------------------------------------------------------------------------------------------\n",
        "from transformers import AutoModel\n",
        "import torch.nn as nn\n",
        "\n",
        "class CustomBERTModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CustomBERTModel, self).__init__()\n",
        "        self.bert = AutoModel.from_pretrained(\"bert-base-uncased\")\n",
        "        self.classifier = nn.Linear(768, 3)  # Example: 3 classes\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        cls_output = outputs.last_hidden_state[:, 0, :]  # [CLS] token embedding\n",
        "        logits = self.classifier(cls_output)\n",
        "        return logits\n",
        "\n",
        "model = CustomBERTModel()\n",
        "\n",
        "\n",
        "--->    This gives full flexibility to design model.\n",
        "\n",
        "\"\"\"\n"
      ],
      "metadata": {
        "id": "ALRg675YhCRV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Mapping tokenization\n",
        "\n",
        "\"\"\"\n",
        "You now want to test out having more control over the tokenization and want to try tokenizing the data in rows or batches.\n",
        "This will also give you a result that is a DataSet object, which you'll need for training.\n",
        "\n",
        "The tokenizer has been loaded for you along with the data as train_data and test_data.\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "Complete tokenize_function returning tokenized tensors with sequence truncation and tokenize the train_data in batches.\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "# Complete the function\n",
        "def tokenize_function(data):\n",
        "    return tokenizer(data[\"interaction\"],\n",
        "                     return_tensors = \"pt\",\n",
        "                     padding=True,\n",
        "                     truncation = True,\n",
        "                     max_length=64)\n",
        "\n",
        "tokenized_in_batches = train_data.map(tokenize_function , batched = True)\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "Apply tokenize_function to train_data and tokenize row by row.\n",
        "\"\"\"\n",
        "\n",
        "# Complete the function\n",
        "def tokenize_function(data):\n",
        "    return tokenizer(data[\"text\"],\n",
        "                     return_tensors=\"pt\",\n",
        "                     padding=True,\n",
        "                     truncation=True,\n",
        "                     max_length=64)\n",
        "\n",
        "# Tokenize row by row\n",
        "tokenized_by_row =  train_data.map(tokenize_function, batched=False)\n",
        "\n",
        "print(tokenized_by_row)"
      ],
      "metadata": {
        "id": "qDb93XzxhC23"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Fine-tuning through Training"
      ],
      "metadata": {
        "id": "53JMWXNT-hta"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### Setting up training arguments\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "Set up an instance of TrainingArguments().\n",
        "Set the evaluation strategy as \"epoch\".\n",
        "Specify three training epochs.\n",
        "Set the batch sizes for both training and evaluation as three.\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "from transformers import Trainer, TrainingArguments\n",
        "\n",
        "\n",
        "# Set up an instance of TrainingArguments\n",
        "training_args = TrainingArguments(\n",
        "  output_dir=\"./finetuned\",\n",
        "\n",
        "  # Set the evaluation strategy\n",
        "  evaluation_strategy = \"epoch\",\n",
        "\n",
        "  # Specify the number of epochs\n",
        "  num_train_epochs=3,\n",
        "  learning_rate=2e-5,\n",
        "\n",
        "  # Set the batch sizes\n",
        "  per_device_train_batch_size=3,\n",
        "  per_device_eval_batch_size=3,\n",
        "  weight_decay=0.01\n",
        ")"
      ],
      "metadata": {
        "id": "Q0OJQmV6-kw4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Setting up the trainer\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "Set up the Trainer() object.\n",
        "Assign the previously defined training arguments and tokenizer.\n",
        "Train the model.\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "# Set up the trainer object\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "\n",
        "    # Assign the training arguments and tokenizer\n",
        "    args = training_args,\n",
        "    train_dataset=tokenized_training_data,\n",
        "    eval_dataset=tokenized_test_data,\n",
        "    tokenizer = tokenizer\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "XMZRBis10oK0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Using the fine-tuned model\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "Tokenize the new data.\n",
        "Pass the tokenized inputs into the fine-tuned model, disabling gradients.\n",
        "Extract the new predictions.\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "\n",
        "input_text = [\"I'd just like to say, I love the product! Thank you!\"]\n",
        "\n",
        "# Tokenize the new data\n",
        "inputs = tokenizer(input_text, return_tensors=\"pt\", padding=True, truncation=True)\n",
        "\n",
        "# Pass the tokenized inputs through the model\n",
        "with torch.no_grad():\n",
        "    outputs = model(**inputs)\n",
        "\n",
        "# Extract the new predictions\n",
        "predicted_labels = torch.argmax(outputs.logits, dim=1).tolist()\n",
        "\n",
        "label_map = {0: \"Low risk\", 1: \"High risk\"}\n",
        "for i, predicted_label in enumerate(predicted_labels):\n",
        "    churn_label = label_map[predicted_label]\n",
        "    print(f\"\\n Input Text {i + 1}: {input_text[i]}\")\n",
        "    print(f\"Predicted Label: {predicted_label}\")"
      ],
      "metadata": {
        "id": "FCl3t9OC09gm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Fine-tuning approaches"
      ],
      "metadata": {
        "id": "ML_22x5fGqfp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "\n",
        "Complete the one-shot learning example by showing the sample review is Positive.\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "# # Include an example in the input ext\n",
        "# input_text = \"\"\"\n",
        "# Text: \"The dinner we had was great and the service too.\"\n",
        "# Classify the sentiment of this sentence as either positive or negative.\n",
        "# Example:\n",
        "# Text: \"The food was delicious\"\n",
        "# ____\n",
        "# Text: \"The dinner we had was great and the service too.\"\n",
        "# Sentiment:\n",
        "# \"\"\"\n",
        "\n",
        "# # Apply the example to the model\n",
        "# result = model(input_text, max_length=100)\n",
        "\n",
        "# print(result[0][\"label\"])\n",
        "\n",
        "\n",
        "# Include an example in the input ext\n",
        "input_text = \"\"\"\n",
        "Text: \"The dinner we had was great and the service too.\"\n",
        "Classify the sentiment of this sentence as either positive or negative.\n",
        "Example:\n",
        "Text: \"The food was delicious\"\n",
        "Sentiment: Positive\n",
        "Text: \"The dinner we had was great and the service too.\"\n",
        "Sentiment:\n",
        "\"\"\"\n",
        "\n",
        "# Apply the example to the model\n",
        "result = model(input_text, max_length=100)\n",
        "\n",
        "print(result[0][\"label\"])"
      ],
      "metadata": {
        "id": "18wDFZfgGmK6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "\n",
        "N-shot learning means training a model to recognize something new based on how many examples it has seen before.\n",
        "\n",
        "\n",
        "Zero-shot learning: The model has never seen the new task before, but it still tries to perform well.\n",
        "\n",
        "Example: A model trained to recognize animals (dogs, cats, etc.) is asked to identify a zebra, even though it has never seen a zebra before.\n",
        "Instead, it uses its knowledge of other animals to make a guess.\n",
        "\n",
        "\n",
        "One-shot learning: The model learns from just one example.\n",
        "\n",
        "Example: If you show a child one picture of a koala, and then they see another koala in a different picture, they can recognize it immediately. One-shot learning works the same way.\n",
        "\n",
        "\n",
        "Few-shot learning: The model learns from a few examples.\n",
        "\n",
        "Example: If a child sees three different pictures of koalas, they can now recognize them better than if they had seen just one.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "One-shot Learning in Practice\n",
        "Imagine you have a text generation model (like ChatGPT) that usually writes essays. But now, you want it to analyze sentiment (happy/sad) of a sentence.\n",
        "\n",
        "Without One-shot Learning:\n",
        "You ask: \"Is this sentence positive or negative?\"\n",
        "\n",
        "The model may not understand that you want sentiment analysis.\n",
        "With One-shot Learning:\n",
        "You give an example before asking:\n",
        "\"Example: 'I love this movie!' → Positive\n",
        "Now analyze: 'I hate this weather!'\"\n",
        "\n",
        "The model now understands what you want and correctly says: \"Negative\"\n",
        "\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "U-8GLvsYHX_H"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}